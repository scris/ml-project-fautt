{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "91MsGMTna_P9"
   },
   "source": [
    "# CBU5201 mini-project submission\n",
    "\n",
    "\n",
    "## What is the problem?\n",
    "\n",
    "This year's mini-project considers the problem of predicting whether a narrated story is true or not. Specifically, you will build a machine learning model that takes as an input an audio recording of **3-5 minutes** of duration and predicts whether the story being narrated is **true or not**. \n",
    "\n",
    "\n",
    "## Which dataset will I use?\n",
    "\n",
    "A total of 100 samples consisting of a complete audio recording, a *Language* attribute and a *Story Type* attribute have been made available for you to build your machine learning model. The audio recordings can be downloaded from:\n",
    "\n",
    "https://github.com/CBU5201Datasets/Deception\n",
    "\n",
    "A CSV file recording the *Language* attribute and *Story Type* of each audio file can be downloaded from:\n",
    "\n",
    "https://github.com/CBU5201Datasets/Deception/blob/main/CBU0521DD_stories_attributes.csv\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## What will I submit?\n",
    "\n",
    "Your submission will consist of **one single Jupyter notebook** that should include:\n",
    "\n",
    "*   **Text cells**, describing in your own words, rigorously and concisely your approach, each implemented step and the results that you obtain,\n",
    "*   **Code cells**, implementing each step,\n",
    "*   **Output cells**, i.e. the output from each code cell,\n",
    "\n",
    "Your notebook **should have the structure** outlined below. Please make sure that you **run all the cells** and that the **output cells are saved** before submission. \n",
    "\n",
    "Please save your notebook as:\n",
    "\n",
    "* CBU5201_miniproject.ipynb\n",
    "\n",
    "\n",
    "## How will my submission be evaluated?\n",
    "\n",
    "This submission is worth 16 marks. We will value:\n",
    "\n",
    "*   Conciseness in your writing.\n",
    "*   Correctness in your methodology.\n",
    "*   Correctness in your analysis and conclusions.\n",
    "*   Completeness.\n",
    "*   Originality and efforts to try something new.\n",
    "\n",
    "(4 marks are given based on your audio submission from stage 1.)\n",
    "\n",
    "**The final performance of your solutions will not influence your grade**. We will grade your understanding. If you have an good understanding, you will be using the right methodology, selecting the right approaches, assessing correctly the quality of your solutions, sometimes acknowledging that despite your attempts your solutions are not good enough, and critically reflecting on your work to suggest what you could have done differently. \n",
    "\n",
    "Note that **the problem that we are intending to solve is very difficult**. Do not despair if you do not get good results, **difficulty is precisely what makes it interesting** and **worth trying**. \n",
    "\n",
    "## Show the world what you can do \n",
    "\n",
    "Why don't you use **GitHub** to manage your project? GitHub can be used as a presentation card that showcases what you have done and gives evidence of your data science skills, knowledge and experience. **Potential employers are always looking for this kind of evidence**. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------- PLEASE USE THE STRUCTURE BELOW THIS LINE --------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B-FAUTT: The Boosting-led First-AUdio-Then-Text Approach for Deceptive Story Detection\n",
    "\n",
    "The full code will be made public on GitHub after the deadline at https://github.com/scris/ml-project-fautt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZaGn4ICrfqXZ"
   },
   "source": [
    "# 1 Author\n",
    "\n",
    "**Student Name**: Tianze Qiu\n",
    "\n",
    "**Student ID**: 221170249\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o38VQkcdKd6k"
   },
   "source": [
    "# 2 Problem formulation\n",
    "\n",
    "## 2.1 Problem\n",
    "\n",
    "We're given a dataset of audio recordings of stories, both in English and in Chinese, and their corresponding labels of deceptive or not. The task is to build a machine learning pipeline that can predict whether a story is deceptive or not based on the audio recording file.\n",
    "\n",
    "## 2.2 Why is it interesting?\n",
    "\n",
    "1. **Real-world application**: The ability to detect deceptive stories has a bunch of real-world applications, for example, in lie detection, fraud detection, fake news detection, etc.\n",
    "2. **Difficulty of the task**: The task is very challenging because it requires us to think of a way that can work fine in only 100 samples and gain reasonable accuracy.\n",
    "3. **Potential for creativity**: The task is open-ended and allows for a lot of creativity in the parts of feature engineering, model selection, and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bPTSuaB9L2jU"
   },
   "source": [
    "# 3 Methodology\n",
    "\n",
    "Overall, we use a boosting-led approach utilizing cherry-picked first-audio-then-text multimodal features for this detection problem. These tasks below are involved during the procedure:\n",
    "\n",
    "## 3.1 Preparation\n",
    "\n",
    "First, we prepare the dataset. We derive a text version from the audio files, then translate and lemmatize it to build a text-based dataset. This dataset is used along the original audio-based one.\n",
    "\n",
    "## 3.2 Feature Engineering\n",
    "\n",
    "Audio features are very high-dimensional, and model will be overfitting if original features are given. Also, it will be helpful is model is also assisted with features extracted from text.\n",
    "\n",
    "These are features used during this process. Their details will be described thoroughly in later sections.\n",
    "\n",
    "### Audio Features\n",
    "\n",
    "- [Main] Filter bank features extracted from audio waveform.\n",
    "- Basic voice characteristics, like energy, zero crossings, kurtosis and skew.\n",
    "- Silence related features, like silence count, duration, and joint features with things like language.\n",
    "\n",
    "### Text Features\n",
    "\n",
    "- [Main] TF-IDF features from lemmatized English text.\n",
    "- Basic linguistic features, like word count, sentence count, stop words, etc.\n",
    "- Other features like word richness and repetition ratio.\n",
    "\n",
    "After manually picking these features, we use a CatBoost model to distinguish the importance of all features. Some least important features are then removed to improve efficiency and eliminate overfitting possibilities.\n",
    "\n",
    "## 3.3 Train Process\n",
    "\n",
    "With these features extracted from both modalities, we do binary classification task to detect deceptive stories (label=1) vs truthful stories (label=0).\n",
    "\n",
    "The 100-sample dataset is split into 76 training and 24 test samples (76/24 split) for best performance. Stratified split is used to maintain class distribution, and Scikit-learn standard scaler is utilized to keep every feature in a uniform scale.\n",
    "\n",
    "Three different models are used:\n",
    "\n",
    "- CatBoost: A gradient boosting algorithm optimized for categorical features, known for its high efficiency and performance with minimal hyperparameter tuning.\n",
    "- Naive Bayes: A probabilistic classifier based on Bayes' theorem, commonly used for its simplicity and effectiveness in text classification tasks, especially when feature independence is assumed.\n",
    "- K-Nearest Neighbors (KNN): A non-parametric method that classifies samples based on the majority vote of the closest neighbors, with distance as the key metric for classification.\n",
    "\n",
    "They're stacked as one classifier with a final logistic regression estimator. And this classifier is fitted with the features and labels of the training subset.\n",
    "\n",
    "## 3.4 Validation Process\n",
    "\n",
    "We evaluate the performance of the model by these metrics:\n",
    "\n",
    "- Accuracy: This measures the proportion of correct predictions (both true positives and true negatives) out of all predictions made. It is a general indicator of the model's effectiveness.\n",
    "- ROC-AUC Score: The Receiver Operating Characteristic Area Under the Curve (ROC-AUC) score assesses the trade-off between true positive rate and false positive rate across different classification thresholds. A higher score indicates better model performance.\n",
    "- Classification Report: This includes precision, recall, F1-score, and support for each class:\n",
    "    - Precision: The proportion of positive predictions that are actually correct. High precision means that when the model predicts a class, it is likely correct.\n",
    "    - Recall: The proportion of actual positives that are correctly identified by the model. High recall means the model successfully identifies most positive instances.\n",
    "    - F1-Score: The harmonic mean of precision and recall, providing a balanced measure when there is an uneven class distribution.\n",
    "    - Support: The number of actual occurrences of each class in the dataset.\n",
    "- Confusion Matrix: A table showing the number of true positives, true negatives, false positives, and false negatives. This helps visualize the types of classification errors made by the model.\n",
    "\n",
    "Accuracy and ROC-AUC Score is the two main metrics used in measurement and comparison in the procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N3BwrtEdLDit"
   },
   "source": [
    "# 4 Implemented ML prediction pipelines\n",
    "\n",
    "Describe the ML prediction pipelines that you will explore. Clearly identify their input and output, stages and format of the intermediate data structures moving from one stage to the next. It's up to you to decide which stages to include in your pipeline. After providing an overview, describe in more detail each one of the stages that you have included in their corresponding subsections (i.e. 4.1 Transformation stage, 4.2 Model stage, 4.3 Ensemble stage)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1nDXnzYLLH6"
   },
   "source": [
    "## 4.1 Transformation stage\n",
    "\n",
    "Describe any transformations, such as feature extraction. Identify input and output. Explain why you have chosen this transformation stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0F5_kI95LuZ2"
   },
   "source": [
    "## 4.2 Model stage\n",
    "\n",
    "Describe the ML model(s) that you will build. Explain why you have chosen them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Ensemble stage\n",
    "\n",
    "Describe any ensemble approach you might have included. Explain why you have chosen them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZQPxztuL9AW"
   },
   "source": [
    "# 5 Dataset\n",
    "\n",
    "Describe the datasets that you will create to build and evaluate your models. Your datasets need to be based on our MLEnd Deception Dataset. After describing the datasets, build them here. You can explore and visualise the datasets here as well. \n",
    "\n",
    "If you are building separate training and validation datasets, do it here. Explain clearly how you are building such datasets, how you are ensuring that they serve their purpose (i.e. they are independent and consist of IID samples) and any limitations you might think of. It is always important to identify any limitations as early as possible. The scope and validity of your conclusions will depend on your ability to understand the limitations of your approach.\n",
    "\n",
    "If you are exploring different datasets, create different subsections for each dataset and give them a name (e.g. 5.1 Dataset A, 5.2 Dataset B, 5.3 Dataset 5.3) .\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qf7GN1aeXJI"
   },
   "source": [
    "# 6 Experiments and results\n",
    "\n",
    "Carry out your experiments here. Analyse and explain your results. Unexplained results are worthless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fSrJCR_cekPO"
   },
   "source": [
    "# 7 Conclusions\n",
    "\n",
    "Your conclusions, suggestions for improvements, etc should go here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 References\n",
    "\n",
    "Acknowledge others here (books, papers, repositories, libraries, tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (Pyodide)",
   "language": "python",
   "name": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
